<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanyuan Qiao</title>

  <meta name="author" content="Yanyuan Qiao">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="google-site-verification" content="HXsBsK6dddZmEiX-hilffnGML0O8ErQGuDQlQt4d1-U" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yanyuan Qiao</name>
                  </p>
                  <p>I am a Postdoctoral Research Fellow working with A.P. <a href="http://www.qi-wu.me/">Qi Wu</a>, at
                    <a href="https://www.adelaide.edu.au/aiml/">Australian
                      Institute for Machine Learning (AIML)</a>, <a href="https://www.adelaide.edu.au/">The University
                      of Adelaide</a>, where I completed my Ph.D. in Computer Science, under the supervision of A.P. <a
                      href="http://www.qi-wu.me/">Qi Wu</a> and Dr. <a
                      href="https://sites.google.com/site/yuankiqi/home">Yuankai Qi</a>. Before coming to Australia, I
                    obtained my master degree from University of Chinese Academy of Sciences, China in 2019 and bachelor
                    degree from Southeast University, China in 2016. 
                  </p>
                  <P>
                    My research interests lie broadly in the field of Vision-and-Language and Embodied AI, especially in Vision-and-Language Navigation.
                  </p>
                  <p>

                  </p>
                  <p style="text-align:center">
                    <a href="mailto:yanyuan.qiao@adelaide.edu.au">Email</a> &nbsp/&nbsp
                    <a href="CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=vsfwX2EAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/YanyuanQiao">Github</a>&nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/yanyuan-qiao/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://twitter.com/YanyuanQiao">Twitter</a>

                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/head.jpeg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/head.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Recent News</heading>
                  <p>
                    [Aug. 2023] I have been awarded ICCV 2023 Doctoral Consortium.
                  </p>
                  <p>
                    [Jul. 2023] Two papers are accepted by ICCV 2023.
                  </p>
                  <p>
                    [Jul. 2023] I will join Australian Institue for Machine Learning, University of Adelaide as a
                    Postdoctoral Research Fellow.
                  </p>
                  <p>
                    [Dec. 2022] One paper is accepted by TPAMI.
                  </p>
                  <p>
                    [Mar. 2022] One paper is accepted by CVPR 2022.
                  </p>
                  <p>
                    [July. 2021] One paper is accepted by ACM MM 2021.
                  </p>
                  <p>
                    [Nov. 2020] One paper is accepted by TMM.
                  </p>
                  <p>
                    [Sept. 2020] I will start my PhD at Australian Institue for Machine Learning, University of
                    Adelaide, Australia.
                  </p>


                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <heading>Research</heading>
                  <p></p>
                  <img src="images/2023-osfda.png" alt="clean-usnob" width="210" height="105">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>Improving Online Source-free Domain Adaptation for Object Detection by Unsupervised Data Acquisition</papertitle>
                  </a>
                  <br>
                  Xiangyu Shi, <u>Yanyuan Qiao</u>, Qi Wu, Lingqiao Liu, Feras Dayoub
                  <br>
                  <a href="https://arxiv.org/pdf/2310.19258.pdf">arxiv</a>
                </td>
              </tr>
              
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <p></p>
                  <img src="images/2023-m3ad.png" alt="clean-usnob" width="210" height="90">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>Multi-Modal Adapter for Medical Vision-and-Language Learning</papertitle>
                  </a>
                  <br>
                  Zheng Yu, <u>Yanyuan Qiao</u>, Yutong Xie, Qi Wu
                  <br>
                  <em>International Workshop on Machine Learning in Medical Imaging (MLMI@MICCAI)</em>, 2023
                  <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-45673-2_39">paper</a>
<!--                   <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-45673-2_39">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2203.11591">arxiv</a> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <p></p>
                  <img src="images/2023-mic.png" alt="clean-usnob" width="210" height="120">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>March in Chat: Interactive Prompting for Remote Embodied Referring Expression</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Yuankai Qi, Zheng Yu, Jing Liu, Qi Wu
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2308.10141">arxiv</a>
<!--                   <br>
                  <a href="https://ieeexplore.ieee.org/document/9880046">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2203.11591">arxiv</a> -->
                  /
                  <a href="https://github.com/YanyuanQiao/MiC">code</a>
                </td>
              </tr>

              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2023-vlnpetl.png" alt="clean-usnob" width="210" height="110">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Zheng Yu, Qi Wu
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_VLN-PETL_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Navigation_ICCV_2023_paper.pdf">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2308.10172">arxiv</a>
<!--                   <br>
                  <a href="https://ieeexplore.ieee.org/document/9880046">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2203.11591">arxiv</a> -->
                  /
                  <a href="https://github.com/YanyuanQiao/VLN-PETL">code</a>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2023-hop+.png" alt="clean-usnob" width="210" height="115">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, Qi Wu
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2023
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10006384">paper</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2022-hop.png" alt="clean-usnob" width="210" height="115">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, Qi Wu
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9880046">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2203.11591">arxiv</a>
                  /
                  <a href="https://github.com/YanyuanQiao/HOP-VLN">code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2021-rgan.png" alt="clean-usnob" width="210" height="120">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>R-GAN: Exploring Human-like Way for Reasonable Text-to-Image Synthesis via Generative Adversarial Networks
                  </papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Qi Chen, Chaorui Deng, Ning Ding, Yuankai Qi, Mingkui Tan, Xincheng Ren, Qi Wu
                  <br>
                  <em>ACM International Conference on Multimedia (ACM MM)</em>, 2021
                  <br>
                  <a
                    href="https://dl.acm.org/doi/10.1145/3474085.3475363">paper</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2020-rec.png" alt="clean-usnob" width="210" height="100">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>Referring Expression Comprehension: A Survey of Methods and Datasets</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Chaorui Deng, Qi Wu
                  <br>
                  <em>IEEE Transactions on Multimedia (TMM)</em>, 2020
                  <br>
                  <a href=https://ieeexplore.ieee.org/abstract/document/9285213>paper</a>
                  /
                  <a href=https://arxiv.org/abs/2007.09554>arxiv</a>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2020-rankvqa.png" alt="clean-usnob" width="210" height="125">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>RANKVQA: Answer Re-ranking for Visual Question Answering</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>*, Zheng Yu*, Jing Liu (*equally contributed)
                  <br>
                  <em>IEEE International Conference on Multimedia and Expo (ICME)</em>, 2020 &nbsp<font color="red"><strong>(Oral)</strong></font>
                  <br>
                  <a href=https://ieeexplore.ieee.org/document/9102814>paper</a>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2020-vcvqa.png" alt="clean-usnob" width="210" height="130">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>VC-VQA: Visual calibration mechanism for Visual Question Answering</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>*, Zheng Yu*, Jing Liu (*equally contributed)
                  <br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2020
                  <br>
                  <a href=https://ieeexplore.ieee.org/document/9190828>paper</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2019-vqa.png" alt="clean-usnob" width="205" height="90">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>Improving Visual Question Answering Using Dropout and Enhanced Question Encoder
                  </papertitle>
                  </a>
                  <br>
                  Zhiwei Fang, Jing Liu, Yong Li, <u>Yanyuan Qiao</u>, Qu Tang, Hanqing Lu
                  <br>
                  <em>Pattern Recognition (PR)</em>, 2019
                  <br>
                  <a
                    href="https://doi.org/10.1016/j.patcog.2019.01.038">paper</a>
                </td>
              </tr>
              </LI>
              </br>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2018-vqa.png" alt="clean-usnob" width="210" height="100">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>Enhancing Visual Question Answering Using Dropout
                  </papertitle>
                  </a>

                  <br>
                  Zhiwei Fang, Jing Liu, <u>Yanyuan Qiao</u>, Qu Tang, Yong Li, Hanqing Lu
                  <br>
                  <em>ACM International Conference on Multimedia (ACM MM)</em>, 2018
                  <br>
                  <a
                    href="https://dl.acm.org/doi/10.1145/3240508.3240662">paper</a>
                </td>
              </tr>
              </LI>
              </br>


            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href='https://clustrmaps.com/site/1bvig'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=200&t=n&d=VV100nOzbf1ki1HQwsvI2yP71bcm4xqQ5tykd-QSwsY'/></a>
                  </p> 
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
