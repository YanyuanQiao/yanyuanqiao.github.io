<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanyuan Qiao</title>

  <meta name="author" content="Yanyuan Qiao">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="google-site-verification" content="HXsBsK6dddZmEiX-hilffnGML0O8ErQGuDQlQt4d1-U" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yanyuan Qiao</name>
                  </p>
                  <p>I am a Postdoctoral Research Fellow working with Asst.Prof. <a href="https://people.epfl.ch/josie.hughes?lang=en">Josie Hughes</a> at <a href="https://www.epfl.ch/en/"> École Polytechnique Fédérale de Lausanne (EPFL)</a>, Switzerland.
                    Previously, I spent years working with A.P. <a href="http://www.qi-wu.me/">Qi Wu</a>, at <a href="https://www.adelaide.edu.au/aiml/">Australian
                      Institute for Machine Learning (AIML)</a>, <a href="https://www.adelaide.edu.au/">The University
                      of Adelaide</a>, where I completed my Ph.D. in Computer Science under the supervision of A.P. <a
                      href="http://www.qi-wu.me/">Qi Wu</a> and Dr. <a
                      href="https://sites.google.com/site/yuankiqi/home">Yuankai Qi</a>. 
                  </p>
                  <P>
                    My research interests lie broadly in the field of Vision-and-Language and Embodied AI, especially in Vision-and-Language Navigation.
                  </p>
                  <p>

                  </p>
                  <p style="text-align:center">
                    <a href="mailto:claire.yanyuan.qiao@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=vsfwX2EAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/YanyuanQiao">Github</a>&nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/yanyuan-qiao/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://twitter.com/YanyuanQiao">Twitter</a>

                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/head.jpeg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/head.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <p>
                    [Aug. 2025] Two papers are accepted by EMNLP 2025.
                  </p>
                  <p>
                    [Jun. 2025] One paper is accepted by ICCV 2025 and two papers are accepted by IROS 2025.
                  </p>
                  <p>
                    [Apr. 2025] <a href="https://arxiv.org/pdf/2409.18800">MiniVLN</a> has been selected as ICRA 2025 Best Paper Award Finalist.
                  </p>
                  <p>
                    [Jan. 2025] One paper is accepted by ICLR 2025 and four papers are accepted by ICRA 2025.
                  </p>
<!--                   <p>
                    [Dec. 2023] I have been awarded the PhD degree and Dean's Commendation for Doctoral Thesis Excellence.
                  </p>
                  <p>
                    [Aug. 2023] I have been awarded ICCV 2023 Doctoral Consortium mentored by Prof. <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a>.
                  </p> -->


                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>



          <tr>
          <h2 style="padding:20px; margin-bottom: -30px;vertical-align:middle">
            Selected Publications
            <a href="https://scholar.google.com/citations?user=vsfwX2EAAAAJ" target="_blank" style="font-size: 16px; margin-left: 12px; font-weight: normal;">[Full List]</a>
          </h2>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2025-navbench.png" alt="clean-usnob" width="210" height="85">
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <papertitle>NavBench: Probing Multimodal Large Language Models for Embodied Navigation</papertitle>
              <br>
              <u>Yanyuan Qiao</u>, Haodong Hong, Wenqi Lyu, Dong An, Siqi Zhang, Yutong Xie, Xinyu Wang, Qi Wu
              <br>
              <a href="https://navbench.github.io/">project</a>
              /
              <a href="https://arxiv.org/pdf/2506.01031">arxiv</a>
            </td>
        </tr>

              
          <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2024-opennav.png" alt="clean-usnob" width="210" height="85">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs</papertitle>
                  <br>
                  <u>Yanyuan Qiao</u>, Wenqi Lyu, Hui Wang, Zixu Wang, Zerui Li, Yuan Zhang, Mingkui Tan, Qi Wu
                  <br>
                  <em>International Conference on Robotics and Automation (ICRA)</em>, 2025
                  <br>
                  <a href="https://sites.google.com/view/opennav/home">project</a>
                  /
                  <a href="https://arxiv.org/pdf/2409.18794">arxiv</a>
                </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2024-minivln.png" alt="clean-usnob" width="210" height="105">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation</papertitle>
                  <br>
                  Junyou Zhu, <u>Yanyuan Qiao</u>, Siqi Zhang, Xingjian He, Qi Wu, Jing Liu
                  <br>
                  <em>International Conference on Robotics and Automation (ICRA)</em>, 2025
                  <br>
                  <a href="https://arxiv.org/pdf/2409.18800">arxiv</a>
                </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2025-grduet.png" alt="clean-usnob" width="210" height="95">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>General Scene Adaptation for Vision-and-Language Navigation</papertitle>
                  <br>
                  Haodong Hong, <u>Yanyuan Qiao</u>, Sen Wang, Jiajun Liu, Qi Wu
                  <br>
                  <em>International Conference on Learning Representations (ICLR)</em>, 2025
                  <br>
                  <a href="https://openreview.net/forum?id=2oKkQTyfz7">paper</a>
                </td>
            </tr>
              

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2024-vlmamba.png" alt="clean-usnob" width="210" height="105">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>VL-Mamba: Exploring State Space Models for Multimodal Learning</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Zheng Yu, Zijia Zhao, Sihan Chen, Mingzhen Sun, Longteng Guo, Qi Wu, Jing Liu
                  <br>
                  <em>NeurIPS Workshop on Efficient Natural Language and Speech Processing</em>, 2024
                  <br>
                  <a href="https://yanyuanqiao.github.io/vl-mamba/">project</a>
                  /
                  <a href="https://arxiv.org/pdf/2403.13600.pdf">arxiv</a>
                  /
                  <a href="https://github.com/ZhengYu518/VL-Mamba">code</a>
                </td>
              </tr>


            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2024-vlncopilot.png" alt="clean-usnob" width="210" height="105">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>LLM as Copilot for Coarse-grained Vision-and-Language Navigation</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Qianyi Liu, Jiajun Liu, Jing Liu, Qi Wu
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2024
                  <br>
                  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf">paper</a>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <p></p>
                  <img src="images/2023-mic.png" alt="clean-usnob" width="210" height="120">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>March in Chat: Interactive Prompting for Remote Embodied Referring Expression</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Yuankai Qi, Zheng Yu, Jing Liu, Qi Wu
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2308.10141">arxiv</a>
                  /
                  <a href="https://github.com/YanyuanQiao/MiC">code</a>
                </td>
              </tr>

              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2023-vlnpetl.png" alt="clean-usnob" width="210" height="110">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Zheng Yu, Qi Wu
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_VLN-PETL_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Navigation_ICCV_2023_paper.pdf">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2308.10172">arxiv</a>
                  /
                  <a href="https://github.com/YanyuanQiao/VLN-PETL">code</a>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2023-hop+.png" alt="clean-usnob" width="210" height="115">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, Qi Wu
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2023
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10006384">paper</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2022-hop.png" alt="clean-usnob" width="210" height="115">
                </td>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <papertitle>HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation</papertitle>
                  </a>
                  <br>
                  <u>Yanyuan Qiao</u>, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, Qi Wu
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9880046">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2203.11591">arxiv</a>
                  /
                  <a href="https://github.com/YanyuanQiao/HOP-VLN">code</a>
                </td>
              </tr>

         


             
              </LI>
              </br>


            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href='https://clustrmaps.com/site/1bvig'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=200&t=n&d=VV100nOzbf1ki1HQwsvI2yP71bcm4xqQ5tykd-QSwsY'/></a>
                  </p> 
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
